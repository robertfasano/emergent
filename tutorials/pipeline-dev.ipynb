{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline class allows us to combine multiple optimization processes into a single process. For example, we could do a coarse grid search to obtain an initial signal, then shrink the bounds to enclose the signal and sample with higher resolution, then finally fit all observed data to a model which predicts the location of the maximum. First, we define a cost function to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cost(state, params):\n",
    "    return -np.exp(-(state['X']-params['x0'])**2)*np.exp(-(state['Y']-params['y0'])**2)\n",
    "params = {'sigma_x': 0.3, 'sigma_y': 0.8, 'x0': 0.3, 'y0': 0.6, 'noise':0.0}\n",
    "\n",
    "def holder_table(state, params):\n",
    "    return - np.abs(np.sin(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function should have two arguments. The \"state\" is a dictionary containing the independent variables, and the \"params\" is a dictionary containing parameters defining the cost evaluation - in this case, 'x0' and 'y0' define the center position of a multivariate Gaussian. The pipeline will minimize the cost function, so make sure you set the sign of the return value appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our initial state and the bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'X': 0, 'Y': 2}\n",
    "bounds = {'X': {'min': -3, 'max': 3}, 'Y': {'min': -3, 'max': 3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll instantiate the pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline\n",
    "pipe = Pipeline(cost, params, state, bounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add blocks to the Pipeline to define the optimization process we want to apply. Let's start with a simple 10x10 grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import GridSearch\n",
    "pipe.add(GridSearch({'Steps': 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've added all of the blocks we want, we can run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic grid search came within 1% of the true minimum with only 102 function evaluations (once when initializing the pipeline, 100 times during optimization, and once more to check the final result). However, as the dimensionality of the parameter space grows, the number of iterations required grows exponentially! A coarse 10-point-per-axis grid search in six dimensions would take a million iterations to complete!\n",
    "\n",
    "For optimization in higher dimensions, a number of other algorithms are included. I will demonstrate these in the following examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based methods\n",
    "The grid search is inefficient because all of the sampled points are chosen before starting the optimization. In many cases, the minimum can be found more efficiently by choosing each subsequent point based on knowledge of the parameter space. The simplest way to do this is to measure the local gradient, take a step downhill, and repeat. This is implemented by the GradientDescent algorithm - let's try it out. We'll rebuild the pipeline from scratch, but make sure you've run the above blocks in the current kernel to define the cost function, initial state, and bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 0s\n",
      "INFO:root:Evaluations: 51\n",
      "INFO:root:Improvement: 570.1%\n"
     ]
    }
   ],
   "source": [
    "from emergent.pipeline import Pipeline, GradientDescent\n",
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(GradientDescent())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientDescent comes about as close to the local minimum in half the experimental iterations of the GridSearch method, and the relative performance should increase in higher dimensional parameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L-BFGS-B algorithm is another option, tending to perform excellently in low dimensional spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 0s\n",
      "INFO:root:Evaluations: 31\n",
      "INFO:root:Improvement: 676.8%\n"
     ]
    }
   ],
   "source": [
    "from emergent.pipeline import Pipeline, LBFGSB\n",
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(LBFGSB())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the Adam optimizer incorporates momentum to more efficiently navigate valleys and saddle points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Adam optimizer does not support bounds!\n",
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 0s\n",
      "INFO:root:Evaluations: 100\n",
      "INFO:root:Improvement: 673.5%\n"
     ]
    }
   ],
   "source": [
    "from emergent.pipeline import Pipeline, Adam\n",
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(Adam())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic methods\n",
    "Gradient-based methods can be inefficient for several reasons:\n",
    "* High-dimensional parameter spaces often have many local minima where the optimizer will get trapped.\n",
    "* Measurement noise will degrade the gradient accuracy, which can cause the optimizer to move in the wrong direction or get stuck in regions where the noise is larger than the local gradient\n",
    "* Gradient computations in N dimensions require 2N function evaluations, which can take a while.\n",
    "\n",
    "An alternative is to use semi-random methods where samples are generated based on knowledge of other samples. For example, the DifferentialEvolution algorithm creates a population of points and carries out generations of \"breeding\" to select for regions of parameter space where the cost function is low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 0s\n",
      "INFO:root:Evaluations: 533\n",
      "INFO:root:Improvement: 676.8%\n"
     ]
    }
   ],
   "source": [
    "from emergent.pipeline import Pipeline, DifferentialEvolution\n",
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(DifferentialEvolution())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ParticleSwarm optimizer similarly initializes a random set of particles which move through space to seek both their best known position and the best known position of the swarm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 0s\n",
      "INFO:root:Evaluations: 112\n",
      "INFO:root:Improvement: 323.5%\n"
     ]
    }
   ],
   "source": [
    "from emergent.pipeline import Pipeline, ParticleSwarm\n",
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(ParticleSwarm())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based methods\n",
    "Online optimization is a powerful strategy, especially for expensive, black-box cost functions. The basic idea is to form a closed loop consisting of:\n",
    "1. Fit a parameter surface to observed data.\n",
    "2. Numerically optimize the modeled surface to generate the next point.\n",
    "3. Repeat until convergence.\n",
    "\n",
    "This can be implemented by importing models and specifying an optimizer to use. Make sure to add a pre-sampling block, like a coarse grid search, so the model has data to train on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "WARNING:root:GradientDescent optimizer does not support bounds!\n",
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 3s\n",
      "INFO:root:Evaluations: 37\n",
      "INFO:root:Improvement: 676.0%\n"
     ]
    }
   ],
   "source": [
    "from emergent.pipeline import Pipeline, GridSearch, GaussianProcess, GradientDescent, GaussianModel\n",
    "import logging as log\n",
    "log.basicConfig(level=log.INFO)\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(GridSearch({'Steps': 5}))\n",
    "# pipe.add(GridSearch({'Steps': 5}))\n",
    "for i in range(10):\n",
    "    pipe.add(GaussianProcess({'Optimizer': GradientDescent()}))\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.77880078, -0.63762815, -0.80856032, -0.90483742, -0.89359735,\n",
       "       -0.77880078, -0.69593431, -0.8824969 , -0.9875778 , -0.97530991,\n",
       "       -0.85001609, -0.67032005, -0.85001609, -0.95122942, -0.93941306,\n",
       "       -0.81873075, -0.56978282, -0.72252735, -0.80856032, -0.79851622,\n",
       "       -0.69593431, -0.42741493, -0.54199419, -0.60653066, -0.59899621,\n",
       "       -0.52204578, -0.9875778 , -0.99990927])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMERGENT integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emergent.pipeline import (Pipeline, Source, GridSearch, GaussianModel, Rescale, DifferentialEvolution, \n",
    "                              ParticleSwarm, GaussianProcess, GradientDescent, Adam, LBFGSB, Prune)\n",
    "from emergent.utilities.containers import DataDict\n",
    "\n",
    "''' Define data source '''\n",
    "hub = network.hubs['hub']\n",
    "hub.range = DataDict({'thing': {'X': {'min': -2, 'max': 2}, 'Y': {'min': -2, 'max': 2}}})\n",
    "thing = hub.children['thing']\n",
    "state = {'thing': {'X': 0.1, 'Y': .9}}\n",
    "bounds = hub.range.copy()\n",
    "experiment = hub.gaussian\n",
    "params = {'sigma_x': 0.3, 'sigma_y': 0.8, 'x0': 0.3, 'y0': 0.6, 'noise':0.0}\n",
    "source = Source(state, bounds, experiment, params)\n",
    "\n",
    "''' Declare and run pipeline.  Allowed blocks are:\n",
    "    Sampling methods: GridSearch, DifferentialEvolution, ParticleSwarm\n",
    "    Models: GaussianModel\n",
    "    Other: Rescale '''\n",
    "pipe = Pipeline(state, network, source=source)\n",
    "\n",
    "# pipe.add(LBFGSB())\n",
    "pipe.add(GridSearch(params={'Steps': 10}))\n",
    "pipe.add(Rescale({'threshold': 0.75}))\n",
    "\n",
    "pipe.add(ParticleSwarm({'Inertia': 0.3, 'Cognitive acceleration': 1, 'Social acceleration': 1}))\n",
    "# pipe.add(DifferentialEvolution())\n",
    "\n",
    "# pipe.add(GridSearch(params={'Steps':10}))\n",
    "pipe.add(GaussianModel(optimizer = DifferentialEvolution()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "points, costs = pipe.run()\n",
    "# pipe.plot()\n",
    "print(points[-1], costs[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "After running a pipeline, we can call the pipe.plot() method to generate a plotting widget on the Dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=pipe\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "# seaborn.set()\n",
    "plt.plot(self.costs, '.k')\n",
    "plt.plot(np.minimum.accumulate(self.costs), '--k')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,100)\n",
    "y = np.linspace(0,1,100)\n",
    "predict_points = np.transpose(np.meshgrid(x,y)).reshape(-1, 2)\n",
    "len(predict_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.emit('test', {'name': 'ParticleSwarm'})\n",
    "network.emit('test', {'name': 'DifferentialEvolution'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "module = getattr(importlib.import_module('emergent.pipeline'), 'GridSearch')\n",
    "module().params['Steps'].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline, GridSearch\n",
    "\n",
    "''' Declare source - replace soon '''\n",
    "hub = network.hubs['hub']\n",
    "bounds = hub.range.copy()\n",
    "params = {'sigma_x': 0.3, 'sigma_y': 0.8, 'x0': 0.3, 'y0': 0.6, 'noise':0.0}\n",
    "state = {'thing': {'X': 1, 'Y': 0}}\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def cost(state, params):\n",
    "    return -np.exp(-(state['thing']['X']-params['x0'])**2)\n",
    "\n",
    "experiment = hub.gaussian\n",
    "pipe = Pipeline(experiment, params,state, bounds, network)\n",
    "\n",
    "\n",
    "\n",
    "pipe.add(GridSearch(params={'Steps': 10}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn how EMERGENT models parameter spaces and finds extrema. First, we'll study how EMERGENT chooses experimental data points to acquire, governed by the sampling method chosen by you, the user. Let's look at the base Sampling class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.samplers.sampling import Sampling\n",
    "Sampling??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define a sampling method, you should override the \\_run() method, as we will look at shortly. For now, understand that this method simply executes a bunch of experiments and returns the points (parameter space coordinates) and costs (experimental results). After the data is acquired, the \\_finish() method allows a model to be fit to the data, then goes to either the first, last, or best point sampled. The Sampling class also implements a standardized plot() method which returns a handle to a 2D plot of the parameter space (currently only 2D spaces are supported)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how to define a sampling method. Consider the Grid sampling class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.samplers import Grid\n",
    "Grid??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, defining new sampling methods is easy - just override the \\_run() method, then define any parameters you don't want to hard-code; these parameters will automatically appear in the GUI to be set before starting a run. In this case, the \\_run() method creates a uniform grid between 0 and 1 in _N_ dimensions, runs the experiment at each point, and returns the results.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Don't worry about the actual ranges - the sampling takes places in a normalized subspace from 0 to 1 along all coordinates, and EMERGENT automatically scales to the ranges defined by the Hub.range dictionary, also displayed in the GUI's network tree.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid sampling can be useful for exploring experiments with 1 or 2 knobs, but the number of iterations required grows exponentially in the number of dimensions. For experiments with many free parameters, we can sample more intelligently with the _online_ sampling method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.samplers import Online\n",
    "Online??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that we attach a model to the sampling process - this could be based on physical intuition or just a general modeler like a Gaussian process. The sampling proceeds as follows:\n",
    "\n",
    "1. Pre-sample some number of randomly generated points.\n",
    "2. Fit the model to all acquired data.\n",
    "3. Numerically minimize the _effective cost_ over the modeled surface to choose the next point.\n",
    "4. Repeat step 3 for each point in a batch, then return to step 2, and repeat.\n",
    "\n",
    "The _effective cost_ is the magic bullet that makes this work so well. Instead of sampling uniformly, we sample intelligently, using all of the knowledge gathered about the experiment to suggest the next point. But what function should we use for the effective cost? A good first guess is the amplitude of the parameter surface itself, i.e. seeking extrema where the signal-to-noise is high. However, if your model can also generate an uncertainty estimate for a point, then it works well to optimize some linear combination of the amplitude and the uncertainty. By iterating through various combinations, we can go from optimizer mode (seeking low amplitude) and explorer mode (seeking high uncertainty), refining the model very quickly.\n",
    "\n",
    "Let's now examine how models are implemented in EMERGENT, starting with the base Model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.models.model import Model\n",
    "Model??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a model, you should override the fit() and predict() methods. The former should fit the model based on all acquired data so far, while the latter should generate a prediction of the experimental result (and possibly the uncertainty) at a given state dictionary. The Model class implements a number of other functions too, including generation of the next_sample() in terms of the explorer/optimizer tradeoff _b_ and the ability to plot() the modeled parameter surface in 2D.\n",
    "\n",
    "Now let's look at a specific implementation of a model, Gaussian process regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.models import GaussianProcess\n",
    "GaussianProcess??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with defining a Sampling subclass, it is very simple to define a new model. In this case, we initialize a regressor in the __init__ method, then override the Model.fit() and Model.predict() methods with the specific syntax required for the GaussianProcessRegressor class imported from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an optimization\n",
    "Now let's try actually running an optimization! First, initialize a demo network by running the Getting Started notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Getting started.ipynb\"\n",
    "import requests\n",
    "base_url = 'http://localhost:6000'\n",
    "while True:\n",
    "    try:\n",
    "        r=requests.get(base_url)\n",
    "        if r.text == 'EMERGENT API':\n",
    "            break\n",
    "    except:\n",
    "        continue\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start a process, we POST a settings dictionary to the /run endpoint containing all of the info necessary to define the process. All of the following is handled much simpler by the GUI, but this tutorial demonstrates the exact steps required to manually launch an optimization through the API. Let's construct a command for a grid search optimization of the \"gaussian\" experiment attached to our hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = network.hubs['hub']\n",
    "hub.range['thing'] = {'X': {'min': -2, 'max': 2}, 'Y': {'min': -2, 'max': 2}}\n",
    "thing = hub.children['thing']\n",
    "state = {'thing': thing.state}\n",
    "settings = {'hub': hub.name, 'state': state}\n",
    "\n",
    "settings['experiment'] = {'name': 'gaussian',\n",
    "                          'params': {'sigma_x': 0.3, \n",
    "                                     'sigma_y': 0.8, \n",
    "                                     'x0': 0.3, \n",
    "                                     'y0': 0.6, \n",
    "                                     'noise':0} }\n",
    "settings['sampler'] = {'name': 'Grid',\n",
    "                       'params': {'Steps': 20, 'Sweeps': 1} }\n",
    "\n",
    "settings['process'] = {'type': 'model', 'end at': 'Best point', 'callback': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit this process via the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.post('http://127.0.0.1:6000/run', json=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we specified the Grid sampler with 20 steps, the optimization was a crude brute force search on a 20x20 grid, converging to near the peak after 400 experimental cycles. \n",
    "\n",
    "## Online optimization\n",
    "We could solve this problem more efficiently with online Gaussian process optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = network.hubs['hub']\n",
    "hub.range['thing'] = {'X': {'min': -2, 'max': 2}, 'Y': {'min': -2, 'max': 2}}\n",
    "thing = hub.children['thing']\n",
    "state = {'thing': thing.state}\n",
    "settings = {'hub': hub.name, \n",
    "            'state': state}\n",
    "settings['experiment'] = {'name': 'gaussian',\n",
    "                          'params': {'sigma_x': 0.3, \n",
    "                                     'sigma_y': 0.8, \n",
    "                                     'x0': 0.3, \n",
    "                                     'y0': 0.6, \n",
    "                                     'noise':0.0} }\n",
    "settings['sampler'] = {'name': 'Online',\n",
    "                       'params': {'Presampled points': 5, \n",
    "                                  'Iterations': 10,\n",
    "                                  'Batch size': 5,\n",
    "                                  'Tolerance': 0,\n",
    "                                  'Mode': 'Hybrid'} }\n",
    "settings['model'] = {'name': 'GaussianProcess',\n",
    "                     'params': {'Amplitude': 1,\n",
    "                                'Length scale': 1,\n",
    "                                'Noise': 0,\n",
    "                                'Leash': 1} }\n",
    "\n",
    "settings['process'] = {'type': 'model', 'end at': 'Best point', 'callback': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.actuate({'hub': {'thing': {'X':0, 'Y':0}}})\n",
    "import requests\n",
    "requests.post('http://127.0.0.1:6000/run', json=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = requests.get('http://127.0.0.1:6000/hubs/hub/samplers').json()\n",
    "requests.get('http://127.0.0.1:6000/hubs/hub/samplers/%s/plot/history'%samplers[len(samplers)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model converged within 1% tolerance much quicker! However, we can do better than this: while Gaussian processes can model virtually any complex experimental parameter space, the training requires fitting many degrees of freedom. If we can restrict the degrees of freedom by suggesting a model that's close to the experimental landscape, we can vastly improve the training time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = network.hubs['hub']\n",
    "thing = hub.children['thing']\n",
    "state = {'thing': thing.state}\n",
    "settings = {'hub': hub.name, \n",
    "            'state': state}\n",
    "settings['experiment'] = {'name': 'gaussian',\n",
    "                          'params': {'sigma_x': 0.3, \n",
    "                                     'sigma_y': 0.8, \n",
    "                                     'x0': 0.3, \n",
    "                                     'y0': 0.6, \n",
    "                                     'noise':0} }\n",
    "settings['sampler'] = {'name': 'Online',\n",
    "                       'params': {'Presampled points': 20, \n",
    "                                  'Iterations': 5,\n",
    "                                  'Batch size': 5,\n",
    "                                  'Tolerance': 0.01} }\n",
    "settings['model'] = {'name': 'Nonlinear',\n",
    "                     'params': {'Leash': 0.25} }\n",
    "\n",
    "settings['process'] = {'type': 'model', 'end at': 'Best point', 'callback': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.actuate({'hub': {'thing': {'X':0, 'Y':0}}})\n",
    "import requests\n",
    "requests.post('http://127.0.0.1:6000/run', json=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our targeted Gaussian model converged with fewer iterations and less computation time per training cycle than the more general Gaussian process model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing results\n",
    "The results of the last two experiments can be accessed through the API. First, we find the process ID corresponding to the run we want to look at; these are listed in chronological order at the following endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "samplers = requests.get('http://127.0.0.1:6000/hubs/hub/samplers').json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the data acquired by the first sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://127.0.0.1:6000/hubs/hub/samplers/%s/plot/data'%samplers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we specified a model to fit in the second process, we can plot not only the raw data but also the modeled surface through the following endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://127.0.0.1:6000/hubs/hub/samplers/%s/plot/model'%samplers[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the parameter space is resolved with decent accuracy and high resolution in the second case, despite the fact that we didn't acquire as much experimental data! If you know the functional form of your experiment's parameter space, you should write a Model and use it to generate points with the Online sampler; if not, you can always use a black-box modeler like GaussianProcess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting and importing models\n",
    "Training models with many degrees of freedom can require a lot of data, especially when the dimensionality of the parameter space is large. Luckily, EMERGENT offers the ability to import weights from previously-trained models as initial guesses to speed up convergence. After running an optimization, we can export the weights using the Model.\\_export() abstract method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.samplers[1].model._export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Model class has a unique file extension labeling its saved weights: for example, the .gpr extension is associated with the GaussianProcess model. We can get all trained weights in the network's data folder with the following endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.get('http://localhost:6000/models/GaussianProcess/weights').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model we exported above by specifying it in the settings dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = network.hubs['hub']\n",
    "hub.range['thing'] = {'X': {'min': -2, 'max': 2}, 'Y': {'min': -2, 'max': 2}}\n",
    "thing = hub.children['thing']\n",
    "state = {'thing': thing.state}\n",
    "settings = {'hub': hub.name, \n",
    "            'state': state}\n",
    "settings['experiment'] = {'name': 'gaussian',\n",
    "                          'params': {'sigma_x': 0.3, \n",
    "                                     'sigma_y': 0.8, \n",
    "                                     'x0': 0.3, \n",
    "                                     'y0': 0.8,\n",
    "                                     'noise':0.0} }\n",
    "settings['sampler'] = {'name': 'Online',\n",
    "                       'params': {'Presampled points': 20, \n",
    "                                  'Iterations': 5,\n",
    "                                  'Batch size': 5,\n",
    "                                  'Tolerance': 0,\n",
    "                                  'Mode': 'Hybrid'} }\n",
    "settings['model'] = {'name': 'GaussianProcess',\n",
    "                     'params': {'Amplitude': 1,\n",
    "                                'Length scale': 1,\n",
    "                                'Noise': 0,\n",
    "                                'Weights': 'model.gp',\n",
    "                                'Leash': 1} }\n",
    "\n",
    "settings['process'] = {'type': 'model', 'end at': 'Best point', 'callback': None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.actuate({'hub': {'thing': {'X':0, 'Y':0}}})\n",
    "import requests\n",
    "requests.post('http://127.0.0.1:6000/run', json=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = requests.get('http://127.0.0.1:6000/hubs/hub/samplers').json()\n",
    "requests.get('http://127.0.0.1:6000/hubs/hub/samplers/%s/plot/history'%samplers[len(samplers)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.hubs['hub'].samplers[1].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

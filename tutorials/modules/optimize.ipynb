{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline class allows us to combine multiple optimization processes into a single process. For example, we could do a coarse grid search to obtain an initial signal, then shrink the bounds to enclose the signal and sample with higher resolution, then finally fit all observed data to a model which predicts the location of the maximum. First, we define a cost function to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cost(state, params):\n",
    "    return -np.exp(-(state['X']-params['x0'])**2)*np.exp(-(state['Y']-params['y0'])**2)\n",
    "params = {'sigma_x': 0.3, 'sigma_y': 0.8, 'x0': 0.3, 'y0': 0.6, 'noise':0.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function should have two arguments. The \"state\" is a dictionary containing the independent variables, and the \"params\" is a dictionary containing parameters defining the cost evaluation - in this case, 'x0' and 'y0' define the center position of a multivariate Gaussian. The pipeline will minimize the cost function, so make sure you set the sign of the return value appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our initial state and the bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'X': 0, 'Y': 2}\n",
    "bounds = {'X': (-3,3), 'Y': (-3,3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll instantiate the pipeline object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline\n",
    "pipe = Pipeline(cost, params, state, bounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add blocks to the Pipeline to define the optimization process we want to apply. Let's start with a simple 10x10 grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import GridSearch\n",
    "pipe.add(GridSearch({'Steps': 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've added all of the blocks we want, we can run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, costs = pipe.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic grid search came within 1% of the true minimum with only 102 function evaluations (once when initializing the pipeline, 100 times during optimization, and once more to check the final result). However, as the dimensionality of the parameter space grows, the number of iterations required grows exponentially! A coarse 10-point-per-axis grid search in six dimensions would take a million iterations to complete!\n",
    "\n",
    "For optimization in higher dimensions, a number of other algorithms are included. I will demonstrate these in the following examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based methods\n",
    "The grid search is inefficient because all of the sampled points are chosen before starting the optimization. In many cases, the minimum can be found more efficiently by choosing each subsequent point based on knowledge of the parameter space. The simplest way to do this is to measure the local gradient, take a step downhill, and repeat. This is implemented by the GradientDescent algorithm - let's try it out. We'll rebuild the pipeline from scratch, but make sure you've run the above blocks in the current kernel to define the cost function, initial state, and bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline, GradientDescent\n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(GradientDescent())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GradientDescent comes about as close to the local minimum in half the experimental iterations of the GridSearch method, and the relative performance should increase in higher dimensional parameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L-BFGS-B algorithm is another option, tending to perform excellently in low dimensional spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline, LBFGSB\n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(LBFGSB())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the Adam optimizer incorporates momentum to more efficiently navigate valleys and saddle points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline, Adam\n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(Adam())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic methods\n",
    "Gradient-based methods can be inefficient for several reasons:\n",
    "* High-dimensional parameter spaces often have many local minima where the optimizer will get trapped.\n",
    "* Measurement noise will degrade the gradient accuracy, which can cause the optimizer to move in the wrong direction or get stuck in regions where the noise is larger than the local gradient\n",
    "* Gradient computations in N dimensions require 2N function evaluations, which can take a while.\n",
    "\n",
    "An alternative is to use semi-random methods where samples are generated based on knowledge of other samples. For example, the DifferentialEvolution algorithm creates a population of points and carries out generations of \"breeding\" to select for regions of parameter space where the cost function is low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline, DifferentialEvolution\n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(DifferentialEvolution())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ParticleSwarm optimizer similarly initializes a random set of particles which move through space to seek both their best known position and the best known position of the swarm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import Pipeline, ParticleSwarm\n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(ParticleSwarm())\n",
    "points, costs = pipe.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based methods\n",
    "Online optimization is a powerful strategy, especially for expensive, black-box cost functions. The basic idea is to form a closed loop consisting of:\n",
    "1. Fit a parameter surface to observed data.\n",
    "2. Numerically optimize the modeled surface to generate the next point.\n",
    "3. Repeat until convergence.\n",
    "\n",
    "Models are implemented as subpipelines which consume real data from the primary pipeline to train, then run through a set of user-defined blocks to numerically optimize the modeled surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import *     \n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(GridSearch({'Steps': 10}))      ## acquire initial data with a coarse grid search\n",
    "model = pipe.add(GaussianProcess())       # attach a model subpipeline\n",
    "grad = model.add(ParticleSwarm())            # run gradient descent to optimize the modeled surface\n",
    "points, costs = pipe.run()      # run through all attached blocks, including the model subpipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training a model, the surface can be visualized by calling Model.plot(axis), where \"axis\" is an integer corresponding to the order of the state dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt     \n",
    "# ^ specify GUI backend for Jupyter compatibility\n",
    "model.plot(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops\n",
    "To loop over a series of actions, you can create a Loop block and attach some other blocks to it. In the following example, we do the following three times:\n",
    "1. Run a 20-step grid search.\n",
    "2. Shrink the bounds to include only points within 50% of the best observed cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emergent.pipeline import *     \n",
    "\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "\n",
    "loop = pipe.add(Loop({'Loops': 1}))\n",
    "loop.add(GridSearch({'Steps': 20}))      ## acquire initial data with a coarse grid search\n",
    "loop.add(Rescale({'Threshold': 0.5}))\n",
    "    \n",
    "points, costs = pipe.run()      # run through all attached blocks, including the model subpipeline\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core integration\n",
    "If you've defined an experiment under control using the Core module, it's simple to optimize it. Let's start up the default demo network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/robbiefasano/emergent/emergent\n",
      "Overwriting /Users/robbiefasano/emergent/emergent/networks/test/things/test_thing.py\n",
      "Overwriting /Users/robbiefasano/emergent/emergent/networks/test/hubs/test_hub.py\n",
      "Overwriting /Users/robbiefasano/emergent/emergent/networks/test/network.py\n",
      "DataDict([('thing', {'X': 1.0, 'Y': 2.0})])\n",
      "New state: DataDict([('thing', {'X': 1, 'Y': 2})])\n",
      " * Serving Flask app \"emergent.API.API\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "%run \"core.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try optimizing the hub's \"gaussian\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Optimization complete!\n",
      "INFO:root:Time: 0s\n",
      "INFO:root:Evaluations: 227\n",
      "INFO:root:Initial cost: -0.000000\n",
      "INFO:root:Final cost: -0.996463\n",
      "INFO:root:Improvement: 3716754001699876.0%\n"
     ]
    }
   ],
   "source": [
    "cost = core.hubs['hub'].gaussian\n",
    "state = {'thing': {'X': 0, 'Y': 5}}\n",
    "bounds = {'thing': {'X': (-1,1), 'Y': (-1,1)}}\n",
    "params = {'sigma_x': 0.3, 'sigma_y': 0.8, 'x0': 0.3, 'y0': 0.6, 'noise':0.0}\n",
    "\n",
    "from emergent.pipeline import Pipeline, GridSearch\n",
    "pipe = Pipeline(cost, params, state, bounds)\n",
    "pipe.add(GridSearch({'Steps': 15}))\n",
    "points, costs = pipe.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now optimizing an experiment rather than an abstract function, the hub's state is now updated to the best identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataDict([('thing', {'X': 0.2857142857142856, 'Y': 0.5714285714285714})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting socketIO client.\n"
     ]
    }
   ],
   "source": [
    "core.hubs['hub'].state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
